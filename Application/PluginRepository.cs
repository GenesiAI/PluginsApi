using AiPlugin.Domain;
using AiPlugin.Infrastructure;

namespace AiPlugin.Application
{
    public class PluginRepository : IPluginRepository
    {
        private readonly AiPluginDbContext dbContext;

        public PluginRepository(AiPluginDbContext dbContext)
        {
            this.dbContext = dbContext;
        }

        public async Task<Plugin> CreatePlugin(Guid userId, string content)
        {

            //split the content in chunks of 700 chars where a . 
            //todo use ai with describe
            var sectionsContent = SplitSection(content);

            var sections = await DescribeSections(sectionsContent);

            var plugin = new Plugin
            {
                UserId = userId,
                OriginalText = content,
                Sections = sections,
                // SchemaVersion = "1.0",
                NameForHuman = "My plugin", //todo use ai to describe the plugin
                NameForModel = "MyPlugin", //todo use ai to describe the plugin
                DescriptionForHuman = "My plugin description", //todo use ai to describe the plugin
                DescriptionForModel = "MyPluginDescription", //todo use ai to describe the plugin
                LogoUrl = "https://em-content.zobj.net/thumbs/120/microsoft/319/puzzle-piece_1f9e9.png",
                ContactEmail = "unknown",
                LegalInfoUrl = "unknown",
            };
            //save the content in the db
            dbContext.Plugins.Add(plugin);
            await dbContext.SaveChangesAsync();
            return plugin;
        }


        public async Task<Plugin> GetPlugin(Guid userId, Guid pluginId)
        {
            var plugin = await dbContext.Plugins.FindAsync(pluginId) ?? throw new KeyNotFoundException("Plugin not found");
            if (plugin.UserId != userId)
                throw new InvalidOperationException("mismatch between userId and pluginId");
            return plugin;
        }

        public async Task<Section> GetSection(Guid userId, Guid pluginId, Guid sectionId)
        {
            var section = await dbContext.Sections.FindAsync(sectionId) ?? throw new KeyNotFoundException("Section not found");
            if (section.PluginId != pluginId)
                throw new InvalidOperationException("mismatch between pluginId and sectionId");
            return section;
        }

        #region private methods

        //temporarily code, TODO soon to use AI to describe the sections and maybe do the splitting too 
        private async Task<IEnumerable<Section>> DescribeSections(IEnumerable<string> sectionsContent)
        {
            var sections = new List<Section>();
            foreach (var sectionContent in sectionsContent)
            {
                var section = new Section
                {
                    Name = "Section " + sections.Count,
                    // Description = "Description of section " + sections.Count,
                    Content = sectionContent
                };
                sections.Add(section);
            }

            return sections;
        }


        
        private IEnumerable<string> SplitSection(string content)
        {
            // simply split at 80K chars
            return Enumerable.Range(0, content.Length / 80000)
                .Select(i => content.Substring(i * 80000, 80000));
            
        }
        //private IEnumerable<string> Test => SplitSection("OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft.[5][6][7]\n\nOpenAI was founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members.[8][1][9] Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a $10 billion investment in 2023.[10][11]\n\nHistory\n2015–2018: Non-profit beginnings\nIn December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Research announced[12] the formation of OpenAI and pledged over $1 billion to the venture. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public.[13][14] OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.[15][3]\n\nAccording to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of the deep learning movement, and drew up a list of the \"best researchers in the field\".[16] Brockman was able to hire nine of them as the first employees in December 2015.[16] In 2016 OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.[16]\n\nMicrosoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect.[16] OpenAI's potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\"[16] Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\"[16] OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.[16]\n\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research.[17] In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.[18][19][20][21]\n\nIn 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone.[22] In comparison, DeepMind's total expenses in 2017 were $442 million. In summer 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.\n\nIn 2018, Musk resigned his board seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla's AI development for self-driving cars.[23] Sam Altman claims that Musk believed OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected. Musk would subsequently leave OpenAI but claimed to remain a donor, yet made no donations after his departure.[24]\n\n2019: Transition to for-profit\nIn 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit capped at 100 times any investment.[25] According to OpenAI, the capped-profit model allows OpenAI LP to legally attract investment from venture funds, and in addition, to grant employees stakes in the company, the goal being that they can say \"I'm going to OpenAI, but in the long term it's not going to be disadvantageous to us as a family.\"[26] Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to.[27] Prior to the transition, public disclosure of the compensation of top employees at OpenAI was legally required.[28]\n\nThe company then distributed equity to its employees and partnered with Microsoft,[29] announcing an investment package of $1 billion into the company. OpenAI also announced its intention to commercially license its technologies.[30] OpenAI plans to spend the $1 billion \"within five years, and possibly much faster.\"[31] Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.[32]\n\nThe transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can't compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be number one.\"\n\nThe nonprofit, OpenAI Inc., is the sole controlling shareholder of OpenAI LP. OpenAI LP, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI Inc.'s nonprofit charter. A majority of OpenAI Inc.'s board is barred from having financial stakes in OpenAI LP.[26] In addition, minority members with a stake in OpenAI LP are barred from certain votes due to conflict of interest.[27] Some researchers have argued that OpenAI LP's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.[33]\n\n2020–present: ChatGPT, DALL-E and partnership with Microsoft\nIn 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering of questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply \"the API\", would form the heart of its first commercial product.[34]\n\nIn 2021, OpenAI introduced DALL-E, a deep learning model that can generate digital images from natural language descriptions.[35]\n\nIn December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days.[36] According to anonymous sources cited by Reuters in December 2022, OpenAI was projecting $200 million revenue in 2023 and $1 billion revenue in 2024.[37]\n\nAs of January 2023, OpenAI was in talks for funding that would value the company at $29 billion, double the value of the company in 2021.[38] On January 23, 2023, Microsoft announced a new multi-year 10 billion USD investment in OpenAI.[39][40]\n\nThe investment is believed to be a part of Microsoft's efforts to integrate OpenAI's ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google's place as a go-to source for information.[41][42]\n\nOn February 7, 2023, Microsoft announced that it is building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.[43]\n\nOn March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest between his board seat at OpenAI and his investments in AI technology companies via Greylock Partners, as well as his role as the co-founder of the AI technology startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.[44]\n\nOn March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.[45]\n\nParticipants\n\nCEO and co-founder of OpenAI, Sam Altman\nKey employees:\n\nCEO and co-founder:[46] Sam Altman, former president of the startup accelerator Y Combinator\nPresident and co-founder:[47] Greg Brockman, former CTO, 3rd employee of Stripe[48]\nChief Scientist and co-founder: Ilya Sutskever, a former Google expert on machine learning[48]\nChief Technology Officer:[47] Mira Murati, previously at Leap Motion and Tesla, Inc.\nChief Operating Officer:[47] Brad Lightcap, previously at Y Combinator and JPMorgan Chase\nBoard of the OpenAI nonprofit:\n\nGreg Brockman\nIlya Sutskever\nSam Altman\nAdam D'Angelo\nWill Hurd\nTasha McCauley\nHelen Toner\nShivon Zilis\nIndividual investors:[48]\n\nReid Hoffman, LinkedIn co-founder[49]\nPeter Thiel, PayPal co-founder[49]\nJessica Livingston, a founding partner of Y Combinator\nCorporate investors:\n\nMicrosoft[50]\nKhosla Ventures[51]\nInfosys[52]\nMotives\nSome scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Co-founder Musk characterizes AI as humanity's \"biggest existential threat.\"[53] Seeking to mitigate the inherent dangers of artificial intelligence, OpenAI's founders structured it as a non-profit so that they could focus its research on making positive long-term contributions to humanity.[14]\n\nMusk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence.[54][55] OpenAI states that \"it's hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\".[14] Research on safety cannot safely be postponed: \"because of AI's surprising history, it's hard to predict when human-level AI might come within reach.\"[56] OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.\"[14] Co-chair Sam Altman expects the decades-long project to surpass human intelligence.[57]\n\nVishal Sikka, former CEO of Infosys, stated that an \"openness\" where the endeavor would \"produce results generally in the greater interest of humanity\" was a fundamental requirement for his support, and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\".[58] Cade Metz of Wired suggests that corporations such as Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook that own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.[57]\n\nStrategy\nMusk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" Musk acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; nonetheless, the best defense is \"to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.\"[48]\n\nMusk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: \"If you have a button that could do bad things to the world, you don't want to give it to everyone.\"[55] During a 2016 conversation about the technological singularity, Altman said that \"we don't plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated that \"Our goal right now... is to do the best thing there is to do. It's a little vague.\"[59]\n\nConversely, OpenAI's initial decision to withhold GPT-2 due to a wish to \"err on the side of caution\" in the presence of potential misuse has been criticized by advocates of openness. Delip Rao, an expert in text generation, stated \"I don't think [OpenAI] spent enough time proving [GPT-2] was actually dangerous.\" Other critics argued that open publication is necessary to replicate the research and to be able to come up with countermeasures.[60]\n\nProducts and applications\nAs of 2021, OpenAI's research focuses on reinforcement learning (RL).[61] OpenAI is viewed as an important competitor to DeepMind.[62]\n\nGym\nAnnounced in 2016, Gym aims to provide an easily implemented general-intelligence benchmark over a wide variety of environments—akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research. It hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible.[17][63] The project claims to provide the user with a simple interface. As of June 2017, Gym can only be used with Python.[64] As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.[65][non-primary source needed]\n\nRoboSumo\nReleased in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and pushing the opposing agent out of the ring.[66] Through this adversarial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.[66][67] OpenAI's Igor Mordatch argues that competition between agents can create an intelligence \"arms race\" that can increase an agent's ability to function, even outside the context of the competition.[66]\n\nVideo game bots and benchmarks\nOpenAI Five\nMain article: OpenAI Five\nOpenAI Five is the name of a team of five OpenAI-curated bots that are used in the competitive five-on-five video game Dota 2, who learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup.[68][69] After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon.[70][71] The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.[72][73][74]\n\nBy June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players.[75][72][76][77] At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games.[78][79][80] In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.[81][82] The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.[83]\n\nGYM Retro\nReleased in 2018, Gym Retro is a platform for RL research on video games.[84] Gym Retro is used to research RL algorithms and study generalization. Prior research in RL has focused chiefly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\n\nDebate Game\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.[85][86]\n\nDactyl\nDeveloped in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects.[87] It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.[88]\n\nIn 2019, OpenAI demonstrated that Dactyl could solve a Rubik's Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik's Cube introduce complex physics that is harder to model. OpenAI solved this by improving the robustness of Dactyl to perturbations; they employed a technique called Automatic Domain Randomization (ADR), a simulation approach where progressively more difficult environments are endlessly generated. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.[89]\n\nAPI\nIn June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task.\"[90][91]\n\nGenerative models\nFurther information: Generative pre-trained transformer\nOpenAI's original GPT model\n\nThe original GPT model\nThe original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI's website on June 11, 2018.[92] It showed how a generative model of language is able to acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\n\nGPT-2\nMain article: GPT-2\n\nAn instance of GPT-2 writing a paragraph based on a prompt from its own Wikipedia article in February 2021\nGenerative Pre-trained Transformer 2 (\"GPT-2\") is an unsupervised transformer language model and the successor to OpenAI's original GPT model. GPT-2 was first announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released out of concern over potential misuse, including applications for writing fake news.[93] Some experts expressed skepticism that GPT-2 posed a significant threat.\n\nThe Allen Institute for Artificial Intelligence responded to GPT-2 with a tool to detect \"neural fake news\".[94] Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\".[95] In November 2019, OpenAI released the complete version of the GPT-2 language model.[96] Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.[97][98][99]\n\nGPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples).\n\nThe corpus it was trained on, called WebText, contains slightly over 8 million documents for a total of 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.[100]\n\nGPT-3\nMain article: GPT-3\nFirst described in May 2020, Generative Pre-trained[a] Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2.[102][103][104] OpenAI stated that full version of GPT-3 contains 175 billion parameters,[104] two orders of magnitude larger than the 1.5 billion parameters[105] in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).[106]\n\nOpenAI stated that GPT-3 succeeds at certain \"meta-learning\" tasks. It can generalize the purpose of a single input-output pair. The paper gives an example of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.[104]\n\nGPT-3 dramatically improved benchmark results over GPT-2. OpenAI cautioned that such scaling up of language models could be approaching or encountering the fundamental capability limitations of predictive language models.[107] Pre-training GPT-3 required several thousand petaflop/s-days[b] of compute, compared to tens of petaflop/s-days for the full GPT-2 model.[104] Like that of its predecessor,[93] GPT-3's fully trained model was not immediately released to the public on the grounds of possible abuse, though OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.[90][109]\n\nOn September 23, 2020, GPT-3 was licensed exclusively to Microsoft.[110][111]\n\nCodex\nMain article: OpenAI Codex\nAnnounced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories,[112][113] and is the AI powering the code autocompletion tool GitHub Copilot.[113] In August 2021, an API was released in private beta.[114] According to OpenAI, the model is able to create working code in over a dozen programming languages, most effectively in Python.[112]\n\nSeveral issues with glitches, design flaws, and security vulnerabilities have been brought up.[115][116]\n\nGitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.[117]\n\nOpenAI announced that they are going to discontinue support for Codex API starting from March 23, 2023.[118][119]\n\nWhisper\n\nReleased in 2022, Whisper is a general-purpose speech recognition model.[120] It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.[121]\n\nGPT-4\nMain article: GPT-4\nOn March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs.[122] OpenAI announced the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers; by contrast, the prior version, GPT-3.5, scored around the bottom 10%. GPT-4 can also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.[123]\n\nUser Interfaces\nMuseNet and Jukebox (music)\nReleased in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with ten different instruments in fifteen different styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays.[124][125] In pop culture, initial applications of this tool were utilized as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.[126][127]\n\nReleased in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".[128][129][130]\n\nMicroscope\nReleased in 2020, Microscope[131] is a collection of visualizations of every significant layer and neuron of eight different neural network models which are often studied in interpretability.[132] Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.[133]\n\nDALL-E and CLIP (images)\nMain article: DALL-E\n\nImages produced by DALL-E when given the text prompt \"a professional high-quality illustration of a giraffe dragon chimera. a giraffe imitating a dragon. a giraffe made of dragon.\"\nRevealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions.[134]\n\nAlso revealed in 2021, CLIP does the opposite: it creates a description for a given image.[135] DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\n\nIn March 2021, OpenAI released a paper titled Multimodal Neurons in Artificial Neural Networks,[136] where they showed a detailed analysis of CLIP (and GPT) models and their vulnerabilities. The new type of attacks on such models was described in this work.\n\nWe refer to these attacks as typographic attacks. We believe attacks such as those described above are far from simply an academic concern. By exploiting the model's ability to read text robustly, we find that even photographs of hand-written text can often fool the model.\n\n— Multimodal Neurons in Artificial Neural Networks, OpenAI\nDALL-E 2\nIn April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results.[137] In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.[138]\n\nChatGPT\nMain article: ChatGPT\nLaunched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.[139]\n\nChatGPT Plus\nChatGPT Plus is a $20/month subscription service which allows users to access ChatGPT during peak hours, provides faster response times, and gives users early access to new features.[140]");
    }
    #endregion
}